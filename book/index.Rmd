--- 
title: "Historias de Crédito y Predicciones de Impuntualidad de Pago"
author: "Michelle Audirac"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "En este documento se presenta un estudio de caso en el que se aplica ciencia de datos para analizar historias de crédito y generar predicciones de impuntualidad."
---

<!-- # Prerequisites -->

<!-- This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$. -->

<!-- The **bookdown** package can be installed from CRAN or Github: -->

<!-- ```{r eval=FALSE} -->
<!-- install.packages("bookdown") -->
<!-- # or the development version -->
<!-- # devtools::install_github("rstudio/bookdown") -->
<!-- ``` -->

<!-- Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`. -->

<!-- To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>. -->

<!-- ```{r include=FALSE} -->
<!-- # automatically create a bib database for R packages -->
<!-- knitr::write_bib(c( -->
<!--   .packages(), 'bookdown', 'knitr', 'rmarkdown' -->
<!-- ), 'packages.bib') -->
<!-- ``` -->

```{r, include=FALSE}
library(feather)
library(tidyverse)
library(mltools)
library(gridExtra)
library(magrittr)
library(scales)

design_matrix <- read_feather('../pipeline/preprocess/output/design_matrix_20180823_prod.feather')
design_matrix %<>%
  filter(pkcolocadora == 21160)

creditos <- read_feather('../pipeline/wrangle_train/output/collapse_train_20200202_dev.feather')
limpieza <- read_csv('../pipeline/wrangle_train/output/limpieza_train_20180823_prod.csv')
```

# Introducción

Este estudio de caso describe un proyecto en el que un equipo de científicos de datos ayudó a una institución otorgadora de crédito a identificar cuándo sus clientes harían pagos impuntuales. En el Capítulo \@ref(contexto) se describe con detalle la motivación y el problema a resolver. 

El entregable del proyecto fue un pipeline de datos que cada quincena actualiza los últimos movimientos en los créditos y genera predicciones de impuntualidad. Con esto en mente, el plan de trabajo que los científicos de datos siguieron fue: 

1. **EDA** se dio sentido a los datos transaccionales para que reflejaran historias de crédito. La limpieza de datos fue extensiva y se incorporó al pipeline una tarea que identifica, limpia y lista errores cada vez que se actualizan los datos.

2. **Feature engineering** se transformaron todas las variables explicativas para que capturaran el comportamiento pasado de los acreditados usando promedios móviles con decaimiento exponencial.

3. **Model selection** se probaron distintos algoritmos de clasificación y se determinaron los mejores hiperparámetros.

4. **Code refactoring** se factorizó el código y se crearon contenedores para cada una de las tareas del pipeline.

Además, se trabajó con ingenieros de datos que estuvieron encargados del **ETL** y del **pipeline testing**. Para el ETL se desarrollaron los procesos de extracción de datos que alimentan al resto de las tareas. Durante el pipeline testing se validaron los resultados que produce el pipeline y se probó el funcionamiento de la entrega.

En el Capítulo \@ref(contexto) se describen las etapas 1 a 3 de este plan de trabajo y en el repositorio [https://github.com/audiracmichelle/prediccion-morosidad/](https://github.com/audiracmichelle/prediccion-morosidad/) se encuentra el código factorizado que se produjo en la etapa 4.

Una vez que se completaron las etapas del plan de trabajo, los científicos de datos dieron seguimiento al modelo puesto en producción. El Capítulo \@ref(conclusiones) cubre qué se utilizó para validar que el desempeño cumplía con lo esperado.

<!-- ## Contacto -->

<!-- Liliana Millán | liliana.millan\@gmail.com | líder del equipo de científicos de datos que trabajó en el proyecto -->

