[
["index.html", "Historias de Crédito y Predicciones de Impuntualidad de Pago Capítulo 1 Introducción", " Historias de Crédito y Predicciones de Impuntualidad de Pago Michelle Audirac 2020-02-25 Capítulo 1 Introducción Este estudio de caso describe un proyecto en el que un equipo de científicos de datos ayudó a una institución otorgadora de crédito a identificar cuándo sus clientes harían pagos impuntuales. En el Capítulo 2 se describe con detalle la motivación y el problema a resolver. El entregable del proyecto fue un pipeline de datos que cada quincena actualiza los últimos movimientos en los créditos y genera predicciones de impuntualidad. Con esto en mente, el plan de trabajo que los científicos de datos siguieron fue: EDA se dio sentido a los datos transaccionales para que reflejaran historias de crédito. La limpieza de datos fue extensiva y se incorporó al pipeline una tarea que identifica, limpia y lista errores cada vez que se actualizan los datos. Feature engineering se transformaron todas las variables explicativas para que capturaran el comportamiento pasado de los acreditados usando promedios móviles con decaimiento exponencial. Model selection se probaron distintos algoritmos de clasificación y se determinaron los mejores hiperparámetros. Code refactoring se factorizó el código y se crearon contenedores para cada una de las tareas del pipeline. Además, se trabajó con ingenieros de datos que estuvieron encargados del ETL y del pipeline testing. Para el ETL se desarrollaron los procesos de extracción de datos que alimentan al resto de las tareas. Durante el pipeline testing se validaron los resultados que produce el pipeline y se probó el funcionamiento de la entrega. En el Capítulo 2 se describen las etapas 1 a 3 de este plan de trabajo y en el repositorio https://github.com/audiracmichelle/prediccion-morosidad/ se encuentra el código factorizado que se produjo en la etapa 4. Una vez que se completaron las etapas del plan de trabajo, los científicos de datos dieron seguimiento al modelo puesto en producción. El Capítulo 4 cubre qué se utilizó para validar que el desempeño cumplía con lo esperado. "],
["contexto.html", "Capítulo 2 Contexto 2.1 La organización 2.2 Problema a Resolver 2.3 Solución Propuesta 2.4 Mecanismo de Validación", " Capítulo 2 Contexto 2.1 La organización La organización en estudio es una SOFOM (Sociedad Financiera de Objeto Múltiple) que realiza otorgamiento de crédito. En México las SOFOMes son una buena opción para aquellos sectores de la población con acceso a financiamiento limitado. Esto es porque la forma en que ofrecen créditos generalmente es más simple, flexible y con menores requisitos que la de las grandes Instituciones financieras. Cabe mencionar que el costo de esta accesibilidad es una tasa de interés más alta. En este trabajo no revelaremos la razón social de la SOFOM en estudio y de aquí en adelante nos referiremos a esta como el Banco. Tampoco se presentarán los datos globales del Banco: como el tamaño de su cartera de crédito o su número de acreditados. Sin embargo, los fines de este trabajo requieren que se describan las características de los créditos que otorga el Banco y en particular nos interesa explicar la forma en que este clasifica el comportamiento de pago de sus clientes. Aunque el Banco no otorga créditos de forma directa en sucursal; tampoco se trata de un negocio de e-lending. La forma en que el Banco presta es a través de comisionistas. No nos concentraremos en explicar la relación entre los comisionistas y el Banco ya que, para nuestros fines, la podemos entender simplemente como la relación de un acreedor con un deudor en un contrato de crédito. Por esto, nos referimos a los comisionistas como clientes o acreditados. Los créditos que el Banco otorga a sus clientes son de tipo revolvente. A diferencia de los créditos tradicionales en los que se presta un valor inicial y se acuerda un plazo y pagos fijos para saldar la deuda, a los créditos revolventes se les atribuye un saldo límite y los clientes pueden disponer del crédito cuando lo requieren. El Banco guarda la información transaccional de sus clientes y, de acuerdo a una evaluación hecha por un equipo de científicos de datos, cuenta con una madurez de datos suficiente para incorporar pipelines de aprendizaje de máquina a sus procesos. 2.2 Problema a Resolver El Banco buscó a un equipo de científicos de datos para trabajar en un proyecto que consistiría en la creación de indicadores de riesgo de sus clientes. A continuación explicaremos la motivación detrás de la creación de dichos indicadores. Por confidencialidad, no se dan a conocer valores específicos del negocio. Previo a iniciar el proyecto, el Banco asegura haber observado una tendencia ascendente en el quebranto de su cartera. Esto significa que hubo un aumento en el número de clientes que dejaban definitivamente de pagar. Por una parte dicho aumento se podía explicar por el crecimiento en la cartera; sin embargo, una idea prevalente en el Banco es que cambios en su estrategia aceleraron la tendencia. Para conocer los cambios de estrategia que hizo el Banco, primero hay que entender el calendario de pagos que utiliza. Como en cualquier crédito revolvente, cada inicio de corte se determina el monto a pagar de cada cliente y la fecha de pago es la fecha límite en la que los clientes deben cubrir el monto que les corresponde. Un pago lo entendemos como un par de datos dado por (fecha de corte, monto a pagar). Figure 2.1: Representación de fechas de corte y fechas de pago Las fechas de corte de los créditos del Banco son quincenales y regularmente caen el 8 de cada mes para pagar el 18, y el 23 para pagar el 3 del siguiente mes. Así el calendario de fechas de corte y pago lo podemos representar en una línea de tiempo como en la figura 2.1. La cobranza preventiva se refiere a aquellas acciones que se realizan antes de la fecha límite de pago para recordar a los acreditados que realicen sus pagos. La cobranza correctiva son las acciones que se disparan después de la fecha de pago para dar seguimiento a los clientes que no cubrieron su monto correspondiente. Créditos con 90 días vencidos pasan a quebranto. De acuerdo con el Banco, anteriormente se decidió disminuir los recursos destinados a la cobranza preventiva. Esa estrategia provocó que eventualmente tuvieran que aumentarse los recursos destinados a la cobranza correctiva y que ultimadamente aumentara el quebranto más de lo esperado. La motivación detrás del proyecto es fortalecer la cobranza preventiva identificando los posibles evento de impago. Además, generar indicadores de riesgo permite crear controles para evitar que clientes en problemas intenten financiar su deuda con más deuda. 2.3 Solución Propuesta Los clientes pueden cubrir su monto a pagar en una sola exhibición o en varios abonos. La fecha en la que los abonos o movimientos cubren el monto a pagar la llamaremos fecha de abono. Dependiendo del momento en que ocurre la fecha de abono el Banco clasifica los pagos en tres: Pago puntual si la fecha de abono ocurre antes de la fecha de pago. Pago impuntual si la fecha de abono se encuentra después de la fecha de pago. Pago en rescate si no se cubrió el monto a pagar antes de la fecha de corte del siguiente período. En la figura 2.2 se esquematiza la clasificación de pagos en una línea de tiempo. Figure 2.2: Clasificación de pagos El equipo de científicos de datos propuso construir un producto de datos que generara dos indicadores de riesgo. El primero, que llamamos el indicador de impuntualidad mide, la probabilidad de que un cliente haga pagos impuntuales en los siguientes tres períodos o cortes. El segundo, el monto en riesgo, es el indicador de impuntualidad ponderado por el saldo actual del cliente. Al ponderar por saldo, el monto en riesgo permite priorizar a los clientes impuntuales que representan un mayor riesgo para el negocio. Considerando datos históricos, cálculamos el número de días que se aleja la fecha de abono de la fecha de pago. A esta diferencia la llamamos el momento de pago. Así los momentos de pago menores o iguales a cero representan pagos puntuales y los mayores a cero pagos impuntuales. En la figura 2.3 tenemos la distribución de los momentos de pago acotados entre -7 y 20. El Banco se refiere a esta distribución como el triángulo de pagos. Como vemos los clientes prefieren pagar un día antes de la fecha de pago, es decir la moda del momento de pago es -1. Figure 2.3: Triángulo de pagos De acuerdo con los científicos de datos, el indicador de impuntualidad debía ser el encargado de predecir si, en los siguientes tres cortes, el momento de pago de un cliente caería en la cola amarilla y roja del tríangulo de pagos. El Banco aceptó la propuesta. 2.4 Mecanismo de Validación En la sección &lt;ref&gt; se detalla como se construyó la etiqueta de impuntualidad con la que se entrenaron modelos supervisados. Por ahora nos basta saber que se obtuvieron etiquetas para cada cliente y para cada corte en los datos históricos del Banco y con esto se obtuvo que el 90.3% de los casos históricos son negativos y el 9.7% positivos. Para nuestro problema de clasificación es posible utilizar cualquiera de los siguientes modelos base: la regla cero (zero rule, 0R or ZeroR) o el muestreo aleatorio. La regla cero consiste en asignar una predicción “cero” en todos los casos. Calculando las métricas de desempeño teóricas para la regla cero tenemos un accuracy de 90.3%, un recall de 0% y una especificidad de 100%. La matriz de confusión (condicionada al actual) correspondiente es: actual positivo negativo predicción positivo 0 0 negativo 100% 100% Para las métricas de desempeño teóricas del muestreo aleatorio tenemos un accuracy de 82.48% (0.903 * 0.903 + 0.097 * 0.097), un recall de 9.7%, una especificidad de 90.3% y la siguiente matriz de confusión (condicionada al actual): actual positivo negativo predicción positivo 9.7% 9.7% negativo 90.3% 90.3% Al contar con estos valores de referencia se estableció que para ponerse en producción el indicador de impuntualidad, este debía alcanzar al menos un nivel de recall de 50% y de especificidad de 93%. El accuracy correspondiente a estos valores de recall y especificidad es de 88.55% (0.93 * 0.9 + 0.097 * 0.5). "],
["proyecto.html", "Capítulo 3 Proyecto 3.1 Historias de Crédito 3.2 Limpieza de datos 3.3 Feature Engineering 3.4 Selección de Modelos 3.5 Desempeño del Modelo", " Capítulo 3 Proyecto 3.1 Historias de Crédito Para poder analizar un portafolio de crédito, debemos entender que un crédito cuenta una historia. Podemos pensar en una historia de crédito como un conjunto de series de tiempo -saldos, pagos- que corresponden a un acreditado. En la figura 3.1 tenemos el saldo de un crédito en el tiempo. Veamos qué nos puede decir la serie de tiempo del saldo sobre este crédito. Figure 3.1: Saldo y mínimo a pagar de un cliente Por un lado, tenemos que la fecha de inicio del crédito fue en enero 2015 y, durante ese año, el cliente fue aumentando su saldo de $1,000 a $100,000 pesos. Por otro lado, podemos ver que durante 2016 el cliente fue saldando su crédito hasta liquidarlo por completo en enero de 2017. En la misma figura se muestra el pago mínimo que el acreditado debía cubrir en cada corte. Cuando un cliente conoce el saldo y el monto a pagar de su crédito en una fecha de corte, este tiene que realizar sus pagos. Veamos distintos comportamientos de pago con ejemplos. Ejemplo 1 Supongamos que el cliente \\(X\\) debe pagar 1,000 pesos en el corte 1. Ya que los clientes pueden cubrir su monto a pagar en varios abonos, el cliente X hizo dos abonos: 250 pesos cuatro días antes de la fecha de pago 1, y 750 pesos dos días después de la fecha de pago 1. En la figura 3.2 vemos representados estos movimientos. En este caso el 25% del pago fue puntual y el 75% impuntual. Además, de acuerdo con la definición de momento de pago de la sección 2.3, tenemos que el momento de pago del cliente \\(X\\) en el corte 1 es de dos días. Figure 3.2: Comportamiento de pago del cliente X Ejemplo 2 El cliente \\(Y\\) debía pagar 1,000 pesos en el corte 1, y 1500 pesos en el corte 2. En la figura 3.3 tenemos los abonos del cliente \\(Y\\). Podemos ver que: El cliente cubrió su monto a pagar del corte 1 con un abono de 1,000 pesos siete días después de la fecha de pago 1. Así, el 100% de su pago fue impuntual y el momento de este pago es 7. El cliente cubrió su monto a pagar del corte 2 con un abono de 750 pesos tres días después de la fecha de pago 2, y con otro abono de 750 pesos ocho días después de la fecha de pago 2. Así, el 50% del pago fue impuntual y el otro 50% fue de rescate. El momento de pago del cliente \\(Y\\) en el corte 2 es de 8 días. Figure 3.3: Comportamiento de pago del cliente Y A continuación veamos en gráficas de series de tiempo la puntualidad de pago de un cliente real. En la figura 3.4 en el recuadro superior vemos el mínimo a pagar y los abonos puntuales, impuntuales o de rescate que hizo un cliente a lo largo del tiempo. En el recuadro inferior tenemos la representación de la puntualidad de estos pagos en valores porcentuales. Figure 3.4: Puntualidad de pago de un cliente A primera vista podemos apreciar que el porcentaje pagado no siempre suma 100%; por ahora ignoraremos esto ya que en la Sección 3.2 se cubre la limpieza de datos. Poniendo atención al comportamiento de pago representado en la imagen tenemos que: El cliente hizo pagos puntuales en la primer mitad del año 2017. A partir de julio 2017 el cliente empieza a hacer pagos impuntuales más frecuentemente. En 2018 el cliente se atrasa frecuentemente en sus pagos y en varios cortes realiza pagos de rescate. 3.2 Limpieza de datos Probablemente el lector ya se ha preguntado si el Banco registra los datos en el formato necesario para visualizar las historias de crédito de sus clientes como en las figuras 3.1 y 3.4. La respuesta es que se tuvieron que adecuar y limpiar extensivamente los datos transaccionales del Banco. El Banco almacena la mayor parte de su información en una base de datos relacional. Esta base contiene casi 1,109 tablas. Los encargados de la base recomendaron a los científicos de datos qué tablas explorar para poder construir las historias de crédito de sus clientes. Los científicos de datos utilizaron principalmente dos tablas: tabla de saldos - tabla en la que se registran saldos y mínimo a pagar de los clientes en cada corte, y tabla de transacciones de pago - tabla en la que se registran todos los movimientos de pago de los clientes. Uno de los retos principales para los científicos de datos en este proyecto fue limpiar y unir las tablas para obtener historias de crédito congruentes. A continuación se describen algunas de las decisiones que se tomaron en la limpieza de datos. Fechas de inicio de las tablas que contienen datos históricos Ya que el esquema de las bases de datos del Banco ha cambiado conforme el negocio crece, se tiene que la fecha de inicio de las tablas no es la misma. La fecha de inicio de la tabla de saldos es noviembre 2008 y la fecha de inicio de la tabla de transacciones de pago es febrero 2013. Con esto, la fecha de inicio de la tabla de historias de crédito que construyó el equipo de científicos de datos es febrero 2013. Períodos activos de los clientes Ya que las tablas de saldos y de transacciones de pago no tienen registros antes de febrero 2013, para identificar fechas de activación de clientes previas a esta fecha se recurrió a otra tabla legacy. La fecha de activación de los clientes era necesaria para calcular adecuadamente la antigüedad de los clientes. Secuencia de estampas de tiempo En algunos casos, la secuencia en una historia de crédito tenía datos faltantes. En lugar de imputar pagos, se decidió eliminar la estampa de tiempo en la historia del cliente cuando sólo faltaba una estampa de forma consecutiva. Cuando se tenían datos faltantes en dos o más estampas consecutivas se decidió asignarle una bandera al cliente y avisarle al Banco que no se podría generar una predicción para ese cliente. La figura 3.5 muestra el histograma del número de estampas consecutivas faltantes en las historias de los clientes excluyendo el cero. Figure 3.5: Histograma de número de estampas faltantes consecutivas Existen clientes que dejaron de tener actividad crediticia por periodos de más de un año. En estos casos se decidió ignorar el pasado de los cliente y considerarlos como nuevos clientes a partir de la fecha en la que volvieron a estar activos. Además, se consideraron como inactivos a los clientes que mantenía un saldo negativo o un saldo menor de 1,000 pesos en varios periodos consecutivos. Clasificación de abonos La tabla de movimientos contiene registros de transacciones pero no contiene una columna en la que se clasifiquen los abonos en puntuales, impuntuales o de rescate. El primer procedimiento que se desarrolló para clasificar los abonos de cada cliente en cada corte tardaba 24 horas. Después de optimizar el uso de memoria, este procedimiento se mejoró y la última versión tarda a lo más diez minutos. Liquidaciones En muchas ocasiones el mínimo abonado y el mínimo a pagar de los clientes no coinciden al centavo. Esto simplemente significa que los clientes normalmente redondean el monto a pagar por comodidad. Por otro lado, un cliente puede querer liquidar antes su deuda haciendo pagos que exceden el mínimo a pagar. En estos casos se decidió acotar el valor máximo del porcentaje puntual, impuntual y de rescate a 1.5. 3.3 Feature Engineering Las series de tiempo de personas con buen historial crediticio comparten ciertos patrones, lo mismo ocurre con personas impuntuales. Debemos diseñar variables que nos permitan identificar mejor dichos patrones. Hasta este momento, entendemos el comportamiento o características de cada crédito en una fecha de corte específico. Sin embargo, esta información no nos aporta información del comportamiento de pago en el pasado. Se construyeron tres tipos de variables que, a partir de las series de tiempo que conforman una historia de crédito, resumen el comportamiento pasado de un cliente. El primer tipo de variable es un promedio ponderado que utiliza una función de decaimiento que permite asignar más peso a la historia reciente del cliente. Los otros dos tipos de variables capturan la magnitud de los cambios y la desviación estándar en las historias de crédito de los clientes. A continuación se describen a más detalle estas tres variedades. 3.3.1 Decaimiento Geométrico y Promedios Móviles Existen muchos métodos para obtener series de pesos con decaimiento que sumen uno. Con algunos métodos se obtienen pesos que decaen demasiado rápido, como el decaimiento exponencial. El half-life es un concepto que ayuda a controlar que tan rápido deben decaer el tamaño de los pesos. Un half-life se alcanza cuando se acumula la mitad de la suma de los pesos. En nuestro contexto el decaimiento ocurre de lo más reciente a lo más lejano. Por ejemplo, un half-life de 3 meses implica que: los pesos asignados en los tres meses más recientes acumulan aproximadamente 0.5, y los pesos para el resto de la serie acumulan aproximadamente 0.5. De esta manera, series de pesos con un half-life de tres meses decaen más rápido que series de pesos con un half-life de un año. En el Anexo 5.1 se incluye el detalle teórico de la construcción de pesos con decaimiento geométrico para lo que se utilizan, precisamente, distribuciones geométricas. A partir de series de decaimiento geométrico generados con ese método y series de saldo de los clientes generamos las variables: saldo_prom_decay series de saldo con decaimiento y half-life de tres meses. saldo_prom_decay3 series de saldo con decaimiento y half-life de tres cortes. Es importante recalcar que no solo se calcularon pesos de decaimiento individualmente para cada cliente. Dado un cliente, se generaron series en cada corte cómo si se estuviera colapsando el pasado en cada paso de su recorrido de crédito. De esta manera las series que generamos son promedios móviles. Finalmente aclaramos que se determinó que para producir una predicción para un cliente, este necesitaba al menos contar con una historia de 6 periodos. La figura 3.6 muestra un ejemplo de las serie de saldos móviles resultantes. Como se observa, el decaimiento incorpora la información del pasado en cada momento y además suaviza o elimina el ruido de la serie original. Además, podemos ver que el half-life de tres meses suaviza mucho más la serie que el half-life de tres cortes. Figure 3.6: Decaimiento con half-life de tres meses y tres cortes 3.3.2 Series de Diferencias y Desviación Estándar Al restar en cada estampa de tiempo el saldo de un cliente menos el saldo de la estampa de tiempo anterior, se obtuvieron series de diferencias en saldo que guardamos en la variable diff_saldo. En cada estampa de tiempo se calculó la desviación estándar con decaimiento del saldo de un cliente. Las series de desviaciones las asignamos a la variable saldo_sd_decay3. En la figura 3.7 tenemos las series de diferencias y de desviación estándar del saldo de un crédito. Figure 3.7: Series de diferencias y desviaciones estandar de un cliente La siguiente lista muestra algunos ejemplos de las variables que se utilizaron en la selección de modelos: antiguedad - antigüedad de los cliente en cada corte. saldoActual - saldo de los cliente en cada corte. dias_despues_fechaPago - momento de pago de los cliente en cada corte. dias_despues_fechaPago_prom_decay3 - promedios móviles del momento de pago histórico, ponderado con decaimiento geométrico y half-life de tres cortes. porc_puntual - porcentaje puntual del pago de los clientes en cada corte. porc_puntual_prom_decay - promedios móviles (con decaimiento geométrico y half-life de tres meses) del porcentaje puntual de los pagos. porc_puntual_sd_decay - desviaciones estándar del porcentaje puntual de los pagos, ponderados con decaimiento geométrico y half-life de tres meses. En el Anexo 5.2 se incluye una lista más amplia de variables. 3.3.3 Etiquetas Recordemos que en la Sección 2.3 se dijo que el indicador de impuntualidad debe predecir si, en los siguientes tres cortes, el momento de pago de un cliente cae en la cola amarilla y roja del tríangulo de pagos. De acuerdo con lo que los representantes del Banco les explicaron a los científicos de datos, es común que los clientes se atrasen uno o dos días en sus pagos por algún contratiempo sin representar un riesgo. Por esto, conjuntamente se decidió que la etiqueta de un cliente en \\(t\\) debía considerar el promedio de los momentos de pago de los siguientes tres cortes \\(t+1\\), \\(t+2\\) y \\(t+3\\); y que la etiqueta sería 1 si el promedio es mayor a 2 y 0 en los demás casos. Se calcularon los promedios de los momentos de pago en los tres cortes siguientes en cada corte y las etiquetas de impuntualidad las guardamos en la variable label_impuntual_2. 3.4 Selección de Modelos Lo primero que trataremos en esta sección es el cross-validation que se utilizó para la selección de modelos. En este caso, los datos son series de tiempo y hay una dependencia de lo que ocurre en el pasado con lo observado después. Como es indispensable hacer un three-way holdout en el que no haya fuga de información, las particiones se hicieron de la siguiente manera: Conjunto de entrenamiento - todos los datos antes de 2017-06-30 que acumulan el 68% del total. Conjunto de validación - los datos entre 2017-07-01 y 2017-12-31 acumulando el 16% del total. Conjunto de prueba - los datos después de 2018-01-01 que acumulan el 16% del total. Como se mencionó en la Sección 2.4 la proporción de positivos y negativos en las etiquetas históricas es de 90.3% y 9.7%. Las proporciones en los tres conjuntos de cross-validation se mantuvieron muy cerca de estos valores. Es claro que estamos tratando con el entrenamiento de un modelo supervisado de clasificación y que es posible probar diferentes modelos como regresión logística, bosques aleatorios o gradient boosting classifiers. Los científicos de datos probaron distintas combinaciones de modelos, hiperparámetros y de variables. El modelo que resultó tener mayor valor predictivo para el Banco fue el Gradient Boosting Classifier. Más sobre la elección de este algoritmo se cubre adelante. Hagamos un ejercicio visual simple que nos permita ver cómo el decaimiento ayuda a capturar el patrones de pago de los clientes. La figura 3.8 contiene dos gráficas de componentes principales. La diferencia en las gráficas es que los puntos se generaron con distintas matrices de rotación: En el recuadro de la derecha se construyó la matriz de rotación con cuatro variables que solo consideran el último corte observado: antiguedad, saldoActual, diff_saldo, dias_despues_fechaPago, y porc_puntual. En el recuadro de la izquierda se construyó la matriz de rotación con variables que incorporan información del pasado con decaimiento: antiguedad, saldo_prom_decay3, dias_despues_fechaPago_prom_decay3, porc_puntual_prom_decay, y porc_puntual_sd_decay. Los puntos en azul claro representan entradas con etiquetas de impuntualidad positivas, los puntos en azul obscuro tienen etiquetas negativas. Figure 3.8: Transformaciones de dos subconjuntos de variables Los puntos claros se separan más de los puntos obscuros en el recuadro izquierdo que en el recuadro derecho. Esto nos da señales de que el decaimiento capturan información sobre el pago futuro de los clientes que no nos proporcionan los datos puntuales de los créditos. Por otro lado, veamos los resultados de tres bosques aleatorios. El primero short se ajustó sobre un grupo de variables de datos puntuales, el segundo mid sobre un grupo de variables de datos con decaimiento y el tercero long con variables tanto puntuales como con decaimiento. En la figura 3.9 tenemos sus curvas de precision-recall. Figure 3.9: Curvas de precision-recall Finalmente, veamos la curva roc de tres algoritmos de clasificación en la figura 3.10: regresión logística con penalización L2, bosque aleatorio y gradient boosting classifier. Los tres algoritmos fueron entrenados sobre el conjunto de datos long con variables de datos puntuales y con decaimiento. Figure 3.10: Curvas roc de tres algoritmos Con esto, ejemplificamos el proceso de selección de modelos que realizaron los científicos de datos. El Gradient Boosting Classifier es el que alcanza el mejor desempeño. 3.5 Desempeño del Modelo En la Sección 2.4 sobre métricas de validación, establecimos que para aprobar un modelo, este debía alcanzar un recall de 50% y una especificidad de 93%. A continuación mostramos las métricas de desempeño del modelo puesto en producción. Ya mencionamos que el modelo elegido fue un Gradient Boosting Classifier. Este tipo de algoritmos son robustos ante la inclusión de variables de ruido (Hastie, Tibshirani, and Friedman 2009) y se entrenó con el conjunto de variables que presentamos en el Anexo 5.2. La selección de hiperparámetros consistió basicamente en elegir la profundidad de los árboles con un grid. El número de árboles quedó determinado por el early stopping que está incorporado al algoritmo utilizado. A continuación mostramos las medidas desempeño del modelo que se obtienen con el corte (threshold) elegido. Por corte nos referimos al valor a partir del cual se le asigna a una predicción el valor de uno o cero. Matriz de confusión actual positivo negativo predicción positivo 5% 2.7% negativo 5% 87.3% recall 50% - Este nivel de recall cumple con las expectativas establecidas por las métricas de validación. precision 66.3% - Ya que el objetivo del modelo de predicción es accionar estrategias de cobranza correctivas es importante tomar en cuenta cuántos recursos se destinan a este esfuerzo. En ese sentido es importante considerar la precisión del modelo elegido para evitar gastos innecesarios. accuracy 92.3% - Este nivel de accuracy excede lo requerido satisfactoriamente. Después de que el Banco aprobó el desempeño del modelo entrenado, se refactorizó el código y se puso en producción en un pipeline De esta manera se empezaron a producir indicadores de impuntualidad para los clientes activos. Además, se empezó a generar el monto en riesgo que equivale al indicador de impuntualidad de los clientes multiplicados por su saldo. Con esto se facilitó priorizar los esfuerzos de cobranza correctiva y enfocar recursos en aquellos clientes con mayor monto en riesgo. Finalmente, presentamos las variables que tienen mayor poder predictivo para el Gradient Boosting Clasifier en el orden de su importancia. La segunda y tercer columnas contienen el valor de las medianas de estas variables en el grupo con etiqueta positiva y en el grupo con etiqueta negativa. Variables con mayor importancia etiqueta positiva etiqueta negativa dias_despues_fechaPago_prom_decay3 1.23 -0.7 dias_mora_prop_decay3 0.44 0 dias_despues_fechaPago 1 0 porc_puntual 0.56 1.04 saldoActual 54,848 54,290 antiguedad 397 548 porc_mora_prom_decay3 0.35 0 Con esto podemos decir que dias_despues_fechaPago_prom_decay3 el promedio con decaimiento de los momentos de pago es la variable que explica más la impuntualidad futura de pago de los clientes. References "],
["conclusiones.html", "Capítulo 4 Conclusiones", " Capítulo 4 Conclusiones En las conclusiones exploraremos algunas ideas finales sobre las pruebas que podrían realizarse una vez puesto el modelo en producción. También hablaremos sobre un resultado interesante de otro proyecto que extiende el alcance del trabajo que presentamos aquí. Una vez que se ha puesto en producción un modelo y al empezarse a generar nuevas predicciones, un cuestionamiento válido consiste en entender si el modelo se desempeña como se esperaba. En la sección 3.5 se presentaron métricas de desempeño puntuales; sin embargo, en otros datos las métricas serán distintas. ¿Cómo pueden compararse las métricas de desempeño sobre las nuevas predicciones con los valores puntuales que mostramos originalmente? Quizá aquí vale la pena sugerir que cuando un modelo va a ponerse en producción tiene sentido obtener el valor esperado de las métricas de desempeño y rangos de posibles valores. Con esto, las contrapartes pueden facilmente validar si el modelo en producción se sigue desempeñando de la misma manera que originalmente se planteó. ¿Cómo producir rangos de valores válidos para las métricas de desempeño? Hacer distribuciones de referencia con bootstrapping es una opción, aunque el problema que se enfrenta aquí es que el conjunto de prueba es finito y su tamaño afecta de forma directa el tamaño del conjunto de entrenamiento; además recordemos que estamos tratando con series de tiempo y el supuesto de independencia no se cumple. Por esto vamos a recurrir a distribuciones paramétricas. Dividimos el conjunto de prueba en diez subconjuntos, validamos que el tamaño de los subconjuntos sea aceptable y obtenemos la precisión en cada uno. Con esto obtenemos diez valores distintos de precisión, como son pocas observaciones, ajustamos una t-student a la transformación logit de los datos. Finalmente, calculamos el percentil 5% y 95% para acumular el 90% de la distribución.Así, el rango de posibles valores de precisión de nuestro modelo se encuentra entre 61% y 72%. Si hacemos lo mismo con el recall, el intervalo resultante va del 37% al 59%. Como vemos, la métrica de recall tiene más varianza que la de precisión y esto es información muy valiosa. En la figura 4.1 incluimos la distribución empírica y la teórica que se ajustó. Figure 4.1: Curvas roc de tres algoritmos No hay que perder de vista que estos rangos de valores no son los intervalos de confianza de las medias de dichas distribuciones y que los intervalos de confianza nos permiten conocer el valor esperado de las métricas de desempeño de todas las predicciones futuras agregadas (Raschka 2018). Al contruir un intervalo de confianza para la media de la precisión tenemos que este se encuentra entre 65% y 68%; y para la media del recall entre 44% y 51%. Por último, platicaremos un resultado interesante derivado de un proyecto relacionado en el que el Banco les pidió a los científicos de datos predecir qué clientes dejarían de pagar y caerían en mora definitiva. Como punto de referencia, recordemos que en este trabajo describimos cómo se obtuvieron series de diferencias y de desviaciones estándard y que al evaluar la importancia de esas variables, en realidad no tenían mucho poder predictivo. Al poner las manos a la obra, los científicos de datos usaron las mismas variables que ya presentamos para entrenar otro Gradient Boosting Classifier con las etiquetas de impago definitivo. Fue una sorpresa ver que el modelo alcanzara casi el mismo desempeño para los impagos que para los pagos impuntuales. Al ver más de cerca, la importancia de las variables era distinta y para ese problema, las variables que incorporan diferencias y desviaciones estándar, tenían más importancia. References "],
["anexo.html", "Capítulo 5 Anexo 5.1 Decaimento Geométrico 5.2 Lista de Variables", " Capítulo 5 Anexo 5.1 Decaimento Geométrico Aquí presentaremos un método para obtener series diarias de pesos monotónicos decrecientes con un half-life de \\(m\\) días. Esto significa que buscamos una serie \\(w = (w^T, w^{T-1}, \\dots, w^1)\\) de pesos diarios tal que \\(w^t \\geq w^{t-1}\\), \\(\\sum w^t = 1\\) y que además se cumplan dos condiciones: \\(\\sum^T_{k=m} w^k \\leq 1/2\\) \\(\\sum^m_{k=1} w^k \\leq 1/2\\) Para esto nos basaremos en una distribución geométrica en donde \\(X\\) que es el número de fracasos antes del primer éxito con soporte en \\(\\{1, 2, 3, \\ldots\\}\\). Si \\(X\\sim\\text{Geom}(p)\\) entonces \\(\\Pr[X = k] = p(1-p)^{k-1}\\) y la probabilidad acumulada hasta \\(x\\) está dada por \\(\\sum^x_{k=1}\\Pr[X=k]\\). Usando la serie geométrica tenemos que \\[ \\begin{aligned} \\sum^x_{k=1}\\Pr[X=k] &amp; = \\sum^x_{k=1}p(1-p)^{k-1} \\\\ &amp;= p\\sum^{x-1}_{k=0}(1-p)^k \\\\ &amp;= p\\frac{1-(1-p)^x}{1-(1-p)}= 1-(1-p)^x \\end{aligned} \\] Dado un half-life \\(m\\), contruyamos una variable aleatoria \\(Y\\) que tiene distribución geométrica truncada y cuya mediana es \\(m\\): Si \\(Y\\) tiene una distribución geométrica truncada, entonces tiene soporte finito \\(\\{1, 2, 3, \\ldots, T\\}\\) y su función de probabilidad es \\(\\Pr[Y = k] = \\Pr[X=k|X \\leq T] = \\frac{1}{C}\\Pr[X=k]\\) donde \\(C=\\sum^T_{k=1}\\Pr[X=k]\\). Esto es \\[ \\begin{aligned} \\Pr[Y=k] &amp; = \\Pr[X=k|X \\leq T] \\\\ &amp; = \\frac{\\Pr[X=k]}{\\sum^T_{k=1}\\Pr[X=k]} &amp; \\\\ &amp; = \\frac{p(1-p)^{k-1}}{\\sum^T_{k=1}p(1-p)^{k-1}} = \\frac{p(1-p)^{k-1}}{1-(1-p)^T} \\end{aligned} \\] Si la mediana de \\(Y\\) es \\(m\\), entonces buscamos aquella \\(p\\) que cumpla que \\(\\sum^m_{k=1}\\Pr[Y=k] = 1/2\\). Esto es \\[ \\begin{aligned} \\sum^m_{k=1}\\Pr[Y=k] &amp; = \\frac{1}{C}\\sum^m_{k=1}\\Pr[X=k] \\\\ &amp; = \\frac{\\sum^m_{k=1}p(1-p)^{k-1}}{\\sum^T_{k=1}p(1-p)^{k-1}} \\\\ &amp; = \\frac{\\sum^{m-1}_{k=0}(1-p)^k}{\\sum^{T-1}_{k=0}(1-p)^k} = \\frac{1-(1-p)^m}{1-(1-p)^T}= 1/2 \\end{aligned} \\] No existe una solución analítica para \\(p\\), pero podemos obtener soluciones numéricas; existirá una solución siempre y cuando \\(m\\in (0,T/2)\\), con \\(p \\to 0\\) si \\(m\\to T/2\\) y \\(p\\to 1\\) si \\(m\\to 0\\). Supongamos que tenemos \\(T = 1000\\) y \\(m = 45\\), ¿bajo qué \\(p\\) la distribución de \\(Y\\) acumula \\(0.5\\) en \\(m\\)? La solución es \\(p=0.0153\\) como se muestra en la 5.1. Figure 5.1: La solución p Con esto, proponemos la serie de pesos \\(w\\) tal que \\(w^T \\rightarrow \\Pr[Y = 1]\\) \\(w^{T-1} \\rightarrow \\Pr[Y = 2]\\) \\(\\vdots\\) \\(w^1 \\rightarrow \\Pr[Y = T]\\) En la figura 5.2 se muestra la serie de pesos con decaimento geométrico con \\(T = 1000\\) y \\(m = 45\\). La línea vertical se encuentra sobre el half-life \\(m\\) donde se acumula el 50% de los pesos. Figure 5.2: Ejemplo con T = 1000 y m = 45 5.2 Lista de Variables A continuación listamos las veinte variables con las que se entrenó el Gradient Boosting Classifier cuyas predicciones arrojaron el mejor desempeño. antiguedad - antigüedad de los cliente en cada corte. saldoActual - saldo de los cliente en cada corte. saldo_prom_decay3 - promedios móviles del saldo histórico, ponderados con decaimiento geométrico y half-life de tres cortes. saldo_sd_decay3 - desviaciones estándar de los saldos históricos, ponderados con decaimiento geométrico y half-life de tres cortes. diff_saldo - diferencias entre el valor del saldo en una fecha de corte y el saldo en la fecha de corte anterior. diff_saldo_prom_decay3 - promedios móviles de las diferencias en el saldo, ponderados con decaimiento geométrico y half-life de tres cortes. diff_saldo_sd_decay3 - desviaciones estándar de las diferencias en el saldo, ponderados con decaimiento geométrico y half-life de tres cortes. dias_despues_fechaPago - momento de pago de los cliente en cada corte. dias_despues_fechaPago_prom_decay3 - promedios móviles del momento de pago histórico, ponderado con decaimiento geométrico y half-life de tres cortes. dias_despues_fechaPago_sd_decay - desviaciones estándar de los momentos de pago históricos, ponderados con decaimiento geométrico y half-life de tres meses. porc_puntual - porcentaje puntual del pago de los clientes en cada corte. porc_puntual_prom_decay - promedios móviles (con decaimiento geométrico y half-life de tres meses) del porcentaje puntual de los pagos. porc_puntual_sd_decay - desviaciones estándar del porcentaje puntual de los pagos, ponderados con decaimiento geométrico y half-life de tres meses. porc_mora - suma del porcentaje impuntual y de rescate en el pago de los clientes en cada corte. porc_mora_prom_decay3 - promedios móviles (con decaimiento geométrico y half-life de tres cortes) del porcentaje en mora de los pagos. porc_mora_sd_decay3 - desviaciones estándar del porcentaje en mora de los pagos, ponderados con decaimiento geométrico y half-life de tres cortes. dias_puntual_prop_decay - proporción (con decaimiento geométrico y half-life de tres meses) de cortes con abonos puntuales. dias_puntual_sd_decay - desviaciones estándar de la proporción de cortes con abonos puntuales, ponderados con decaimiento geométrico y half-life de tres mesesm dias_mora_prop_decay3 - proporción (con decaimiento geométrico y half-life de tres cortes) de cortes con abonos impuntuales o de rescate. dias_mora_sd_decay3 - desviaciones estándar de la proporción de cortes con abonos impuntuales o de rescate de los clientes, ponderados con decaimiento geométrico y half-life de tres cortes. En el repositorio https://github.com/audiracmichelle/prediccion-morosidad/ se encuentra el pipeline con el que se construyen todas estas variables. "]
]
